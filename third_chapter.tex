\documentclass[]{template}

\begin{document}
~
\section{生成事件描述}
\subsection{本章概述}
在前两章，我们首先验证了事件描述对事件成功的影响，以及衡量了事件描述与事件参与人数的关系。后者对于事件组织者尤为重要，因为一个组织者总是会关心他的活动会有多少人参加。而另一个事件组织者十分感兴趣的话题是他该如何写出能够吸引人的事件描述。这便是本章想要解决的问题。

本章在上一章的基础上，设计了一个生成器(\textit{generator})，用来生成事件描述。在考察了诸多文本生成器后，我们在本文使用了变分自编码器\ref{kingma_auto-encoding_2013,bowman_generating_2015}(\textit{variational autoencoder})。这里使用变分自编码器的原因是因为我们希望文本在编码空间内服从我们期望的分布，这样的话可以避免传统编码器所带来的一些问题。然后我们使用上一章设计的基于GRU的神经网络作为判别器(\textit{discrimitor})，组成一个生成对抗网络\ref{goodfellow_generative_2014}。通过设计合理的奖励函数，来训练生成器生成高质量的事件描述。

本章的结构如下：首先我们会详细介绍生成器的设计和训练过程，然后我们会对本章使用的生成对抗网络以及奖励函数进行详述，最后我们会在真实的\textit{meetup}数据集上给出实验结果。

\subsection{事件描述生成器}
\subsubsection{文本生成器}
循环神经网络语言模型(\ref{RNNLM})是目前十分流行的一种语言模型。在生成句子时，RNNLM仅根据当前隐层的状态给出下一个词的概率分布，而不是其他假设。而RNN模型本身又十分强大，几乎可以拟合任何分布。而自然语言在某种程度上又可以看成一个概率模型，每一个词出现的概率都由之前已出现的词决定，因此，只要有训练数据，RNNLM就可以很好的对复杂文本序列建模。反应在实验中就是其生成的文本十分像它的训练数据，例如如果我们使用莎士比亚的文章作为训练数据，那么其生成文本读起来也像莎士比亚。这种性质也让它在文本生成器中得到了广泛应用，几乎所有的序列到序列(\textit{seq2seq})模型中的解码器(\textit{decoder})都采用了RNNLM。本文也不例外。
\subsubsection{变分自编码器}
前面提到，RNNLM在生成文本序列中的某一个词时，完全依赖于之前前输入的词和隐状态。而在实现中，通常会使用一个特殊符号来作为每一句的开头，因此，RNNLM生成的文本序列完全依赖于其刚开始时的隐状态，即RNN如何初始化。所以如何提供合适的隐状态来使生成的句子符合预期，就显得尤为重要了。一个解决方案是使用第二章提到的GRU编码器。编码器首先将输入序列编码到一个隐空间，然后使用一个前向神经网络将这条编码转换成RNNLM初始化的隐状态。这也不是不可以，但是这样做有一个非常明显的缺陷：我们无法控制输入样本经过编码器编码后在隐空间中的分布。例如在原空间相似的两个文本序列在编码后它们所对应的隐向量可能并不相近。这显然不是我们期望的。更为甚者，由于前面提到，RNN模型可以拟合几乎任何概率模型，所以无论编码器给出什么样的结果，RNNLM在训练的时候都能给出一个不错的结果，但到了测试的时候，就不容乐观了。因此，在这里仅依靠GRU编码器是不够的。但是我们可以使用变分编码器，它是传统的RNN编码器的改进型，它对隐空间中的编码\(\overrightarrow{z}\)加入了先验分布，并在目标函数中通过kl散度来缩小实际分布和先验分布的距离，以此来强迫编码器学到合适的编码方式。






\end{document} 
