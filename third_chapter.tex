\documentclass[]{template}

\begin{document}
~
\section{生成事件描述}
\subsection{本章概述}
在前两章，我们首先验证了事件描述对事件成功的影响，以及衡量了事件描述与事件参与人数的关系。后者对于事件组织者尤为重要，因为一个组织者总是会关心他的活动会有多少人参加。而另一个事件组织者十分感兴趣的话题是他该如何写出能够吸引人的事件描述。这便是本章想要解决的问题。

本章在上一章的基础上，设计了一个生成器(\textit{generator})，用来生成事件描述。在考察了诸多文本生成器后，我们在本文使用了变分自编码器\ref{kingma_auto-encoding_2013,bowman_generating_2015}(\textit{variational autoencoder})。这里使用变分自编码器的原因是因为我们希望文本在编码空间内服从我们期望的分布，这样的话可以避免传统编码器所带来的一些问题。然后我们使用上一章设计的基于GRU的神经网络作为判别器(\textit{discrimitor})，组成一个生成对抗网络\ref{goodfellow_generative_2014}。通过设计合理的奖励函数，来训练生成器生成高质量的事件描述。

本章的结构如下：首先我们会详细介绍生成器的设计和训练过程，然后我们会对本章使用的生成对抗网络以及奖励函数进行详述，最后我们会在真实的\textit{meetup}数据集上给出实验结果。

\subsection{事件描述生成器}
\subsubsection{文本生成器}
循环神经网络语言模型(\ref{RNNLM})是目前十分流行的一种语言模型。在生成句子时，RNNLM仅根据当前隐层的状态给出下一个词的概率分布，而不是其他假设。而RNN模型本身又十分强大，几乎可以拟合任何分布。而自然语言在某种程度上又可以看成一个概率模型，每一个词出现的概率都由之前已出现的词决定，因此，只要有训练数据，RNNLM就可以很好的对复杂文本序列建模。反应在实验中就是其生成的文本十分像它的训练数据，例如如果我们使用莎士比亚的文章作为训练数据，那么其生成文本读起来也像莎士比亚。这种性质也让它在文本生成器中得到了广泛应用，几乎所有的序列到序列(\textit{seq2seq})模型中的解码器(\textit{decoder})都采用了RNNLM。本文也不例外。
\subsubsection{变分自编码器}
前面提到，RNNLM在生成文本序列中的某一个词时，完全依赖于之前前输入的词和隐状态。而在实现中，通常会使用一个特殊符号来作为每一句的开头，因此，RNNLM生成的文本序列完全依赖于其刚开始时的隐状态，即RNN如何初始化。所以如何提供合适的隐状态来使生成的句子符合预期，就显得尤为重要了。一个解决方案是使用第二章提到的GRU编码器。编码器首先将输入序列编码到一个隐空间，然后使用一个前向神经网络将这条编码转换成RNNLM初始化的隐状态。这也不是不可以，但是这样做有一个非常明显的缺陷：我们无法控制输入样本经过编码器编码后在隐空间中的分布。例如在原空间相似的两个文本序列在编码后它们所对应的隐向量可能并不相近。这显然不是我们期望的。更为甚者，由于前面提到，RNN模型可以拟合几乎任何概率模型，所以无论编码器给出什么样的结果，RNNLM在训练的时候都能给出一个不错的结果，但到了测试的时候，就不容乐观了。因此，在这里仅依靠GRU编码器是不够的。但是我们可以使用变分编码器，它是传统的RNN编码器的改进型，它对隐空间中的编码\(\overrightarrow{z}\)加入了先验分布，并在目标函数中通过kl散度来缩小实际分布和先验分布的距离，以此来强迫编码器学到合适的编码方式。变分自编码器的损失函数$L_i(\theta,\phi)$如\ref{3-1}
\begin{equation}\label{3-1}
    L_i(\theta,\phi)=-E_{z\sim q_\theta(z|x_i)}[\log p_\phi(x_i|z)]+KL (q_\theta(z|x_i)||p(z))
\end{equation}
可以看出，其损失函数由两部分构成，第一部分是负对数似然损失函数，用来缩小输入序列和输出序列的差异。第二个部分则是kl散度，其中$q_\theta$为编码器，$p(z)$为对隐编码$z$的先验分布。

\subsubsection{事件描述生成器}
我们参考了\ref{bowman_generating_2015}，设计了如下事件描述生成器\ref{}。在编码器和解码器的选择上，我们都使用了单层GRU。在实现过程中，我们使用$0-1$高斯分布作为隐编码的先验分布。同时使用了重采样\ref{kingma_auto-encoding_2013}的技巧，这样我们便可以用反向传播来训练我们的网络。在训练中，我们使用随机梯度下降的算法，在抽样的时候，我们不直接对隐编码进行抽样，而是通过两个线性神经网络获得当前编码的平均值$\bar{u}$标准差$\bar{v}$，然后通过公式\ref{3-2}获得隐编码，其中$\bar\epsilon \sim Normal(0,1)$。
\begin{equation}\label{3-2}
    z=\bar{u}+\bar{v}\odot \bar\epsilon
\end{equation}

至于如何使用前半段编码结果来初始化解码器中的隐状态，我们尝试了三种方法1）将隐编码连接在解码器的输入词向量的最后。2）将隐编码通过一个前向网络转换成解码器的隐状态向量，并用它直接初始化后者。3）两者皆做。在实际运用过程中，我们发现这三种方法并没有很大的差别，因此最终我们采用了第一种方案。

在实际的训练过程中，为了防止损失函数中KL散度降为0，我们还参考了\ref{bowman_generating_2015}中所采用的策略：在训练刚开始时设置KL散度项的权重为零，然后慢慢升到一。这样训练过程其实就分成了两个阶段：第一个阶段，编码器从文本序列中学到尽可能多的信息，但不保证分布符合先验分布。第二阶段，通过增加KL散度项的权重，强迫编码器编得的隐编码尽可能接近先验分布。

\subsection{生成对抗网络}






\end{document} 
